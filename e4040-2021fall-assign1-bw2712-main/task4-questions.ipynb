{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Task 4: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "A 2 layer MLP is sufficient to model most functions. Why are deep networks used instead of 2 layer networks? \n",
    "\n",
    "   Your answer: **With deeper network, the model could introduce more variable and generate a more complex function to fit the problem. Indeed, deep networks could face overfitting problem but with the increase of size of input data, deep network can achieve better performance and solve this problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "What are the differences between stochastic gradient descent and Batch gradient descent? Why is mini-batch gradient descent used in practice?\n",
    "\n",
    "   Your answer: **The differences are mainly the way to update parameter. (1) Stochastic gradient descent (SGD) update the parameter once every time it get a example from training set with gradient based on this single example. but batch gradient descent update the parameter after computing the gradient for all samples in the dataset. (2) The learning rate of batch gradient descent is fixed, but for Stochastic gradient descent, the learning rate decrease after each update.<br><br>The reason why mini-batch gradient descent used in practice is: <br>(A) Compare with SGD: (1) It reduce the variance of the parameter updates as using batch can provide a more accurate estimation of the gradient; (2) Vectorization can speed up computations.<br>(B) Compare with Batch gradient descent: (1) It can converge quickly as it update the parameter with small batch of data rather than the total dataset. (2) It introduce some regulization effect as the gradient of small batch has some noise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Why are activation functions used in deep learning? What are the issues that can arise when using sigmoid or tanh activation functions?\n",
    "\n",
    "   Your answer: **The reasons to introduce activation function are: (1) Introduce non-linearity. Imaging there is a fully connected neural network with multiple layer with 1 neuron but without activation function, the output can be simplified as $\\theta x$ where $x$ is the input and $\\theta$ is the multiplicity of all weights in each layer. So no matter how deep the network is, it can be simplified as 1 layer network. The problem can be solved with non-linearity; (2) Non-linearity enable network to return multiple type of outputs based on the problem. For instance, the sigmoid can be used in the output layer to get probability (in range [0,1]) and unit step function can be used in binary classification problem; (3) Activition function can also limit value in certain range such as [0,1] avoiding the output to explode.<br><br> The issues using sigmoid are (1) It is computationally expensive as the use of exponential function; (2) It can cause gradient vanishing problem. Considering squeezing real life data with wide range of values into [0,1], the output can be very close to 0 or 1 where have extremly small gradients. Therefore, if poorly initialize the initial parameter, the gradient will be small can change little in the gradient which cause a poor training performance; (3) It is not zero-centered but using normalized data can get a faster convergence. Compared with sigmoid, tanh only fixed the (3) problem but (1) and (2) are still the issues.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "What are the differences between linear and logistic regression? Are both linear?\n",
    "\n",
    "   Your answer: **In general, linear regression is used for regression problems and logistic regression can be used for both regression and classification problems. Linear regression is used to find a line that best-fit the data, which could exceed [0,1] range. Since logistic regression use sigmoid function to introduce non-linearity and put the output value in [0,1] (which can be treated as probability sometimes).<br><br> Linear regression is obviously linear as the output is a linear function of input. The linearities of logistic regression depend on the problem they are applied to solve. When doing classification, logistic regression is linear because of two reasons: (1) The decision boundary of logistic regression is linear (output = 0.5). Set $output = \\frac{1}{1+e^{-\\theta x}}=0.5$, we have $x = -\\theta x$ which is linear; (2) The output can be written with linear terms which is $\\theta x$ in this case. However for the regression problem, as logistic regression introduce sigmoid, the output is not get from a linear function so it is non-linear.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  hyperparameters, which stategies you used to improve the network, show the results of intermediate and the final steps.\n",
    "\n",
    "   Your answer: **I tuned hyperparameters (num_epoch, hidden_dim, batch_size, lr, momentum) with several potential values and pass in different combinations to the model to get the \"optimal\" parameter setting with highest test accuracy. The detailed is showed in task2-mlp-numpy.ipynb. After getting the parameter setting, I used it to retrain the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coding for hyperparameters tunning\n",
    "param_list = {\n",
    "    'num_epoch': [10, 15, 20, 25],\n",
    "    'hidden_dim': [300, 400, 500, 600],\n",
    "    'batch_size': [300, 400, 500, 600],\n",
    "    'lr': [1e-4, 2e-4, 5e-4, 1e-3],\n",
    "    'momentum': [0.5, 0.1, 0.05]\n",
    "}\n",
    "\n",
    "best_param_list={\n",
    "    'test acc': 0,\n",
    "    'num_epoch': 0,\n",
    "    'hidden_dim': 0,\n",
    "    'batch_size': 0,\n",
    "    'lr': 0,\n",
    "    'momentum': 0,\n",
    "}\n",
    "for num_epoch in param_list['num_epoch']:\n",
    "    for hidden_dim in param_list['hidden_dim']:\n",
    "        for batch_size in param_list['batch_size']:\n",
    "            for lr in param_list['lr']:\n",
    "                for momentum in param_list['momentum']:\n",
    "                    model = TwoLayerNet(input_dim=X_train.shape[1], hidden_dim=hidden_dim, num_classes=10, reg=1e-4, weight_scale=1e-3)\n",
    "                    verbose = True\n",
    "                    train_acc_hist, val_acc_hist = train(model, X_train, y_train, X_val, y_val, \n",
    "                                                          num_epoch=num_epoch, batch_size=batch_size, learning_rate=lr, \n",
    "                                                            optim='SGD Momentum', momentum=momentum, verbose=verbose)\n",
    "                    test_score = test(model, X_test, y_test)\n",
    "                    if test_score > best_param_list['test acc']:\n",
    "                        best_param_list['test acc'] = test_score\n",
    "                        best_param_list['num_epoch'] = num_epoch\n",
    "                        best_param_list['hidden_dim'] = hidden_dim\n",
    "                        best_param_list['batch_size'] = batch_size\n",
    "                        best_param_list['lr'] = lr\n",
    "                        best_param_list['momentum'] = momentum\n",
    "print(best_param_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "(Optional, this question is included in the bonus) In tSNE, describe the motivation of tuning the parameter and discuss the difference in results you see.\n",
    "    \n",
    "   Your answer: **In tSNE, I tuned perplexity from 10 to 100 and choose 40 which with best performance to be the value of this hyperparameter. The motivation of tuning this is perplexity balances arrention between local and global perspects of the data. The points in each cluster using small perplexity is more sparse and be more concentrated for large perplexity. However, when perplexity becomes too large, some cluster will be \"tear apart\" and scatter in the image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
